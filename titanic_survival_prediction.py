# -*- coding: utf-8 -*-
"""Titanic Survival Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zeBFFKnOOft6eD8B9PYfQmZjKKXZdlrB
"""

"""==============================================================================
* Project: Titanic Survival Prediction
* Author: Your Gemini Tutor

* Description: This script walks through a complete data science project using the Kaggle Titanic dataset. It covers data loading, EDA, feature engineering, model building with hyperparameter tuning, evaluation, and a conceptual deployment strategy.
# ==============================================================================

"""

# ==============================================================================
# STAGE 1: Data Loading, Exploration & EDA
# ==============================================================================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
import joblib
import warnings
warnings.filterwarnings('ignore')

titanic_df = pd.read_csv('titanic.csv')

train_df = titanic_df.iloc[200:,]

train_df.info()

test_df = titanic_df.iloc[:200,]
test_df.info()

print("Starting the Titanic Survival Prediction project...\n")

# # Load the datasets
# # Assumes 'train.csv' and 'test.csv' are in the same directory.
# try:
#     train_df = pd.read_csv('train.csv')
#     test_df = pd.read_csv('test_df.csv')
#     print("Data loaded successfully.")
# except FileNotFoundError:
#     print("Error: train.csv or test.csv not found. Please ensure the files are in the same directory.")
#     exit()

# Display initial data information
print("\n--- Initial Data Exploration ---")
print("Training data shape:", train_df.shape)
print("Testing data shape:", test_df.shape)
print("\nFirst 5 rows of the training data:")
print(train_df.head())
print("\nData types and missing values in training data:")
print(train_df.info())

# --- Exploratory Data Analysis (EDA) ---
print("\n--- Exploratory Data Analysis (EDA) ---")

# Visualize survival distribution
plt.figure(figsize=(6, 4))
sns.countplot(x='Survived', data=train_df, palette='viridis')
plt.title('Survival Count (0 = No, 1 = Yes)')
plt.show()
print("Interpretation: The majority of passengers did not survive.")

# Survival rate by Sex
plt.figure(figsize=(6, 4))
sns.barplot(x='Sex', y='Survived', data=train_df, palette='pastel')
plt.title('Survival Rate by Gender')
plt.show()
print("Interpretation: Females had a much higher survival rate than males.")

# Survival rate by Passenger Class (Pclass)
plt.figure(figsize=(8, 5))
sns.barplot(x='Pclass', y='Survived', data=train_df, palette='rocket')
plt.title('Survival Rate by Passenger Class')
plt.show()
print("Interpretation: First-class passengers had a significantly higher survival rate.")

# Distribution of Age and its relation to Survival
plt.figure(figsize=(10, 6))
sns.histplot(train_df, x='Age', hue='Survived', multiple='stack', bins=30, kde=True, palette='coolwarm')
plt.title('Age Distribution and Survival')
plt.show()
print("Interpretation: Younger passengers (especially children) and some older passengers seem to have higher survival rates.")

# Correlation Heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(train_df.corr(numeric_only=True), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of Numerical Features')
plt.show()
print("Interpretation: Pclass, Fare, and SibSp have some correlation with Survival.")

# ==============================================================================
# STAGE 2: Data Preprocessing & Feature Engineering
# ==============================================================================
print("\n--- Data Preprocessing & Feature Engineering ---")

# Combine datasets for consistent preprocessing
combined_df = pd.concat([train_df.drop('Survived', axis=1), test_df], ignore_index=True)

# Handle missing values
# Age: Fill missing values with the median age
combined_df['Age'].fillna(combined_df['Age'].median(), inplace=True)
# Fare: Fill missing value with the median fare
combined_df['Fare'].fillna(combined_df['Fare'].median(), inplace=True)
# Embarked: Fill missing values with the most frequent port
combined_df['Embarked'].fillna(combined_df['Embarked'].mode()[0], inplace=True)

# Feature Engineering
# Create a new feature 'FamilySize'
combined_df['FamilySize'] = combined_df['SibSp'] + combined_df['Parch'] + 1
# Create a new feature 'IsAlone'
combined_df['IsAlone'] = (combined_df['FamilySize'] == 1).astype(int)
# Extract Title from Name
combined_df['Title'] = combined_df['Name'].str.extract(' ([A-Za-z]+)\.', expand=False)
# Group less common titles
combined_df['Title'] = combined_df['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')
combined_df['Title'] = combined_df['Title'].replace('Mlle', 'Miss')
combined_df['Title'] = combined_df['Title'].replace('Ms', 'Miss')
combined_df['Title'] = combined_df['Title'].replace('Mme', 'Mrs')

# Drop unnecessary features
combined_df.drop(['Name', 'Ticket', 'Cabin', 'PassengerId'], axis=1, inplace=True)

# Convert categorical features to numerical using one-hot encoding
combined_df = pd.get_dummies(combined_df, columns=['Sex', 'Embarked', 'Title'], drop_first=True)

print("\nPreprocessed data head:")
print(combined_df.head())
print("\nPreprocessed data info:")
print(combined_df.info())

# Split the data back into training and testing sets
X_train_clean = combined_df.iloc[:len(train_df)]
X_test_clean = combined_df.iloc[len(train_df):]
y_train = train_df['Survived']

# ==============================================================================
# STAGE 3: Model Building, Training, and Evaluation with Hyperparameter Tuning
# ==============================================================================
print("\n--- Model Building, Training, and Evaluation with Hyperparameter Tuning ---")

# Split training data for validation
X_train, X_val, y_train_split, y_val = train_test_split(
    X_train_clean, y_train, test_size=0.2, random_state=42
)

# Define the model and the parameter grid for tuning
model = RandomForestClassifier(random_state=42)
param_grid = {
    'n_estimators': [25, 75, 100, 200],
    'max_depth': [3, 5, 8],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Initialize GridSearchCV
print("\nPerforming Grid Search for best hyperparameters...")
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)
grid_search.fit(X_train, y_train_split)

# Get the best model and its parameters
best_model = grid_search.best_estimator_
print("\nGrid Search completed.")
print(f"Best parameters found: {grid_search.best_params_}")
print(f"Best cross-validation score: {grid_search.best_score_:.4f}")

# Make predictions on the validation set using the best model
y_pred = best_model.predict(X_val)

# Evaluate the model's performance
print("\n--- Model Evaluation on Validation Data ---")
print(f"Accuracy: {accuracy_score(y_val, y_pred):.4f}")
print("\nClassification Report:\n", classification_report(y_val, y_pred))

# Perform Cross-Validation on the best model to get a more robust score
print("\n--- Cross-Validation Score of the Best Model ---")
cv_scores = cross_val_score(best_model, X_train_clean, y_train, cv=5)
print(f"5-Fold Cross-Validation Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")

# ==============================================================================
# STAGE 4: Conceptual Deployment
# ==============================================================================
print("\n--- Conceptual Deployment Strategy ---")

# Step 1: Save the trained model
# Using joblib is a standard practice to save the model to a file.
model_filename = 'titanic_model_tuned.joblib'
joblib.dump(best_model, model_filename)
print(f"\nModel saved as '{model_filename}'. This is the file you would deploy.")

# # Step 2: Explain a simple deployment structure (e.g., a Flask API)
# # This code block is a conceptual example and not meant to be run as-is.
# print("\n--- Example of a simple Flask API for deployment ---")
# print("""
# from flask import Flask, request, jsonify
# import joblib
# import pandas as pd

# app = Flask(__name__)

# # Load the trained model
# model = joblib.load('titanic_model_tuned.joblib')

# # Define a function to preprocess new data based on the training logic
# def preprocess_data(data):
#     # This function would contain the same preprocessing steps as above,
#     # ensuring new data is formatted correctly before prediction.
#     # For a real application, you would make this robust.
#     df = pd.DataFrame(data)
#     # The columns need to match the trained model's features.
#     # One-hot encoding needs to be handled carefully for unseen categories.
#     # This is a simplified example.
#     df['FamilySize'] = df['SibSp'] + df['Parch'] + 1
#     df['IsAlone'] = (df['FamilySize'] == 1).astype(int)
#     # Handle categorical variables (Sex, Embarked, Title) with dummy variables
#     df = pd.get_dummies(df, columns=['Sex', 'Embarked', 'Title'], drop_first=True)
#     return df

# @app.route('/predict', methods=['POST'])
# def predict():
#     try:
#         # Get JSON data from the request
#         data = request.get_json()

#         # Preprocess the data
#         processed_data = preprocess_data(data)

#         # Make a prediction
#         prediction = model.predict(processed_data)[0]

#         # Return the prediction as a JSON response
#         return jsonify({'survival_prediction': int(prediction)})
#     except Exception as e:
#         return jsonify({'error': str(e)})

# if __name__ == '__main__':
#     # For a real deployment, you would not use debug=True
#     # and would use a production-ready server like Gunicorn.
#     app.run(debug=True)
# """)
# print("Note: The Flask code above is a conceptual template and requires installation of Flask.")
# print("To run it, you would install Flask (`pip install Flask`) and save the code in a new file (e.g., `app.py`).")
# print("Then, you would run `python app.py` from your terminal.")

# print("\nProject finished. You have successfully completed all the major stages.")