{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68f26392",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b880567",
   "metadata": {},
   "source": [
    "OCR Project: Processing Receipts and Bills\n",
    "This document outlines the design and a case study for a deep learning project focused on processing receipts and bills. The goal is to extract key, structured information from unstructured images, such as a photo of a receipt.\n",
    "\n",
    "Case Study: A Personal Finance Management App\n",
    "Problem Statement: Users of a personal finance app want to automatically track their expenses by simply taking a photo of a receipt. Manual data entry is slow and prone to errors. The challenge is to accurately extract critical information like the merchant, date, total amount, and individual line items from a variety of receipt formats.\n",
    "\n",
    "Why this is a Deep Learning Problem:\n",
    "\n",
    "Variability: Receipts from different merchants have vastly different layouts, fonts, and colors. A simple rule-based system would fail to generalize.\n",
    "\n",
    "Noise: Photos often have glare, shadows, are taken at an angle, or are crumpled.\n",
    "\n",
    "Semantic Understanding: Extracting the \"total amount\" requires more than just reading numbers; it requires understanding the context of the text (e.g., distinguishing the total from a subtotal or tax).\n",
    "\n",
    "Project Design: A Python-Based OCR Pipeline\n",
    "Our solution will be a multi-stage pipeline that combines classical computer vision techniques with modern deep learning for information extraction.\n",
    "\n",
    "1. Image Preprocessing üñºÔ∏è\n",
    "The first step is to clean up the input image to improve the accuracy of the OCR engine.\n",
    "\n",
    "Input: A raw image file (e.g., PNG, JPEG).\n",
    "\n",
    "Steps:\n",
    "\n",
    "Grayscale Conversion & Binarization: Convert the image to black and white to improve text contrast.\n",
    "\n",
    "Noise Reduction: Use a filter (e.g., a median filter) to remove random noise.\n",
    "\n",
    "Deskewing: Correct any rotational distortion caused by taking the photo at an angle.\n",
    "\n",
    "Libraries: OpenCV or Pillow are ideal for these tasks.\n",
    "\n",
    "2. Text Detection and Recognition (OCR) üîç\n",
    "This stage identifies all text within the preprocessed image and converts it into a digital format.\n",
    "\n",
    "Text Detection: A model identifies bounding boxes around all text regions.\n",
    "\n",
    "Text Recognition: An OCR engine reads the characters within each bounding box.\n",
    "\n",
    "Output: A list of strings containing all the detected text, often with their coordinates. For example: ['UBER', 'Invoice', 'Date: 25/08/2025', 'Total: $12.50'].\n",
    "\n",
    "Libraries: Pytesseract (a Python wrapper for the Tesseract engine) is a great starting point due to its simplicity. For a more advanced approach, you could integrate a model like TrOCR from the Hugging Face transformers library.\n",
    "\n",
    "3. Information Extraction with Deep Learning (NLP) üß†\n",
    "This is the most critical and challenging part of the project, where we use deep learning to make sense of the extracted text.\n",
    "\n",
    "Input: The raw text strings from the OCR stage.\n",
    "\n",
    "Method: We will frame this as a Named Entity Recognition (NER) problem. We will fine-tune a pre-trained NLP model (like BERT or a simpler model from spaCy) to recognize specific \"entities\" in the text, such as:\n",
    "\n",
    "MERCHANT (e.g., \"UBER\", \"Starbucks\")\n",
    "\n",
    "DATE (e.g., \"08/26/2025\", \"26-Aug-2025\")\n",
    "\n",
    "TOTAL (e.g., \"$12.50\", \"150.00\")\n",
    "\n",
    "Output: A structured data object (e.g., JSON or a Python dictionary).\n",
    "\n",
    "{\n",
    "  \"merchant\": \"UBER\",\n",
    "  \"date\": \"25/08/2025\",\n",
    "  \"total_amount\": 12.50\n",
    "}\n",
    "\n",
    "Libraries: spaCy with its built-in NER capabilities is a fantastic choice, as is the Hugging Face transformers library, which allows you to fine-tune powerful models like BERT on your specific dataset.\n",
    "\n",
    "Python Project Flow\n",
    "Your main Python script would follow this general structure:\n",
    "\n",
    "main.py\n",
    "\n",
    "Import Libraries: os, cv2 (for OpenCV), pytesseract, spacy (or transformers).\n",
    "\n",
    "Function process_receipt_image(image_path):\n",
    "\n",
    "Takes the image file path as an argument.\n",
    "\n",
    "Loads the image using OpenCV or Pillow.\n",
    "\n",
    "Applies all the preprocessing steps from Stage 1.\n",
    "\n",
    "Runs OCR on the cleaned image to get the raw text (Stage 2).\n",
    "\n",
    "Passes the raw text to a trained NLP model for NER (Stage 3).\n",
    "\n",
    "Returns the final structured JSON data.\n",
    "\n",
    "Main Execution Block:\n",
    "\n",
    "Check for the existence of the image file.\n",
    "\n",
    "Call process_receipt_image() with a test image.\n",
    "\n",
    "Print the resulting structured data.\n",
    "\n",
    "This project is a perfect mix of different deep learning fields and provides a clear path to building a useful, real-world application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3f9203",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab98e0d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72c16bd3",
   "metadata": {},
   "source": [
    " # stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e0e4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A script for the image preprocessing stage of the OCR project.\n",
    "# This code will take an image file, clean it up, and save the result.\n",
    "# We will use OpenCV for image manipulation.\n",
    "\n",
    "# To install: pip install opencv-python numpy\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    \"\"\"\n",
    "    Performs a series of preprocessing steps on a receipt image.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): The path to the input image file.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The preprocessed image as a NumPy array.\n",
    "    \"\"\"\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Error: Image file not found at {image_path}\")\n",
    "        return None\n",
    "\n",
    "    # Load the image\n",
    "    # cv2.imread loads the image in BGR format by default\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Error: Could not read image from {image_path}\")\n",
    "        return None\n",
    "\n",
    "    print(\"Image loaded successfully. Starting preprocessing...\")\n",
    "\n",
    "    # 1. Convert to grayscale\n",
    "    # Grayscale conversion simplifies the image and is often the first step for OCR.\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    print(\"Step 1: Converted to grayscale.\")\n",
    "\n",
    "    # 2. Apply a median filter to remove noise\n",
    "    # Median filter is effective at removing salt-and-pepper noise while preserving edges.\n",
    "    denoised = cv2.medianBlur(gray, 3) # The kernel size (e.g., 3) must be odd\n",
    "    print(\"Step 2: Applied median blur for noise reduction.\")\n",
    "\n",
    "    # 3. Apply thresholding (binarization)\n",
    "    # This step converts the grayscale image to a binary (black and white) image,\n",
    "    # making the text stand out clearly from the background.\n",
    "    # We use Otsu's method for automatic thresholding.\n",
    "    _, binarized = cv2.threshold(denoised, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    print(\"Step 3: Applied Otsu's thresholding for binarization.\")\n",
    "    \n",
    "    # 4. Deskewing (Advanced, optional but recommended)\n",
    "    # This is a more complex step to correct for angled photos.\n",
    "    # We will find the minimum area rectangle enclosing the text and rotate the image.\n",
    "    coords = np.column_stack(np.where(binarized < 255))\n",
    "    angle = cv2.minAreaRect(coords)[-1]\n",
    "    \n",
    "    # The angle returned by minAreaRect is in the range [-90, 0)\n",
    "    # We need to adjust it to get a correct rotation angle\n",
    "    if angle < -45:\n",
    "        angle = -(90 + angle)\n",
    "    else:\n",
    "        angle = -angle\n",
    "        \n",
    "    (h, w) = img.shape[:2]\n",
    "    center = (w // 2, h // 2)\n",
    "    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    deskewed = cv2.warpAffine(binarized, M, (w, h),\n",
    "                              flags=cv2.INTER_CUBIC,\n",
    "                              borderMode=cv2.BORDER_REPLICATE)\n",
    "    \n",
    "    print(f\"Step 4: Deskewed image by {angle:.2f} degrees.\")\n",
    "    \n",
    "    return deskewed\n",
    "\n",
    "def save_image(image, output_path):\n",
    "    \"\"\"\n",
    "    Saves the preprocessed image to a specified path.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cv2.imwrite(output_path, image)\n",
    "        print(f\"Preprocessed image saved to: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving image: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # You would replace this with a real image path.\n",
    "    # For a sample, you can download a receipt image and place it in the same\n",
    "    # directory as this script.\n",
    "    \n",
    "    # Example usage:\n",
    "    # 1. Create a dummy image for demonstration purposes\n",
    "    dummy_img = np.zeros((400, 600, 3), dtype=np.uint8)\n",
    "    cv2.putText(dummy_img, 'Sample Receipt', (100, 200), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "    cv2.imwrite('dummy_receipt.png', dummy_img)\n",
    "    \n",
    "    input_image_path = 'dummy_receipt.png'\n",
    "    output_image_path = 'preprocessed_receipt.png'\n",
    "    \n",
    "    preprocessed_img = preprocess_image(input_image_path)\n",
    "    \n",
    "    if preprocessed_img is not None:\n",
    "        save_image(preprocessed_img, output_image_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09532c60",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f513e8d",
   "metadata": {},
   "source": [
    "# stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa73d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A script for the text recognition stage of the OCR project.\n",
    "# This code will use PyTesseract to extract text from a preprocessed image.\n",
    "\n",
    "# To install: pip install pytesseract Pillow\n",
    "# You must also have the Tesseract OCR engine installed on your system.\n",
    "# See documentation for installation instructions:\n",
    "# Windows: https://github.com/UB-Mannheim/tesseract/wiki\n",
    "# macOS: brew install tesseract\n",
    "# Ubuntu/Debian: sudo apt update && sudo apt install tesseract-ocr\n",
    "\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Note: If Tesseract is not in your system's PATH, you need to set the path\n",
    "# to the tesseract executable.\n",
    "# pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    \"\"\"\n",
    "    Performs preprocessing on an image. This is the same function from the\n",
    "    previous step, included here for a complete, runnable example.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Error: Image file not found at {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        return None\n",
    "\n",
    "    # Grayscale conversion\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Denoising with median blur\n",
    "    denoised = cv2.medianBlur(gray, 3)\n",
    "\n",
    "    # Binarization using Otsu's method\n",
    "    _, binarized = cv2.threshold(denoised, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Deskewing\n",
    "    coords = np.column_stack(np.where(binarized < 255))\n",
    "    angle = cv2.minAreaRect(coords)[-1]\n",
    "    if angle < -45:\n",
    "        angle = -(90 + angle)\n",
    "    else:\n",
    "        angle = -angle\n",
    "    \n",
    "    (h, w) = binarized.shape[:2]\n",
    "    center = (w // 2, h // 2)\n",
    "    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    deskewed = cv2.warpAffine(binarized, M, (w, h),\n",
    "                              flags=cv2.INTER_CUBIC,\n",
    "                              borderMode=cv2.BORDER_REPLICATE)\n",
    "    \n",
    "    return deskewed\n",
    "\n",
    "def ocr_image(image_data):\n",
    "    \"\"\"\n",
    "    Extracts text from an image using PyTesseract.\n",
    "\n",
    "    Args:\n",
    "        image_data (numpy.ndarray): The preprocessed image as a NumPy array.\n",
    "\n",
    "    Returns:\n",
    "        str: A single string containing all the extracted text.\n",
    "    \"\"\"\n",
    "    if image_data is None:\n",
    "        return \"\"\n",
    "\n",
    "    # Convert the OpenCV image (numpy array) to a PIL Image object\n",
    "    # This is often necessary for pytesseract to work correctly\n",
    "    pil_image = Image.fromarray(image_data)\n",
    "\n",
    "    print(\"Running OCR on the preprocessed image...\")\n",
    "\n",
    "    # Use pytesseract.image_to_string() to perform OCR.\n",
    "    # We can pass custom configuration options here, for example to\n",
    "    # improve accuracy on specific document types.\n",
    "    # config = '--psm 6' # PSM 6 assumes a single uniform block of text\n",
    "    # config = '--psm 4' # PSM 4 assumes a single column of text of variable sizes\n",
    "    \n",
    "    extracted_text = pytesseract.image_to_string(pil_image, lang='eng')\n",
    "\n",
    "    # Return the extracted text\n",
    "    return extracted_text\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # We use a dummy image for demonstration purposes.\n",
    "    # In a real application, you would use a receipt image.\n",
    "    dummy_img_path = 'dummy_receipt.png'\n",
    "    \n",
    "    # Preprocess the dummy image first\n",
    "    preprocessed_img = preprocess_image(dummy_img_path)\n",
    "\n",
    "    if preprocessed_img is not None:\n",
    "        # Pass the preprocessed image to the OCR function\n",
    "        extracted_text = ocr_image(preprocessed_img)\n",
    "        print(\"\\n--- Extracted Text ---\")\n",
    "        print(extracted_text)\n",
    "\n",
    "        # You can also get more detailed information, including bounding boxes\n",
    "        # This is very useful for visualizing where the text was found.\n",
    "        # We will need this for the next stage (Information Extraction)\n",
    "        ocr_data = pytesseract.image_to_data(preprocessed_img, output_type=pytesseract.Output.DICT)\n",
    "        print(\"\\n--- OCR Data with Bounding Boxes ---\")\n",
    "        print(ocr_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de32f5aa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f52e3da1",
   "metadata": {},
   "source": [
    "# stage 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f667ded5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A script for the Information Extraction stage of the OCR project.\n",
    "# This code will process the output from PyTesseract to extract specific\n",
    "# key-value pairs like total, tax, and date from a receipt.\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_receipt_info(ocr_data):\n",
    "    \"\"\"\n",
    "    Extracts key information from OCR data using a rule-based approach.\n",
    "\n",
    "    This function simulates the logic of a simple deep learning model\n",
    "    by searching for keywords and their corresponding values. In a real\n",
    "    deep learning solution, a model (e.g., LayoutLM) would be trained to\n",
    "    predict the categories (e.g., 'TOTAL', 'DATE') of each text block\n",
    "    and its associated value.\n",
    "\n",
    "    Args:\n",
    "        ocr_data (dict): The output from pytesseract.image_to_data(),\n",
    "                         containing 'text', 'left', 'top', 'width', and 'height'.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the extracted information.\n",
    "    \"\"\"\n",
    "    if 'text' not in ocr_data:\n",
    "        print(\"Error: Invalid OCR data format. 'text' key is missing.\")\n",
    "        return {}\n",
    "    \n",
    "    # Store the extracted information\n",
    "    extracted_info = {\n",
    "        'total': None,\n",
    "        'subtotal': None,\n",
    "        'tax': None,\n",
    "        'date': None\n",
    "    }\n",
    "    \n",
    "    # Clean the text and create a unified list of words with their coordinates\n",
    "    words_and_coords = []\n",
    "    for i, word in enumerate(ocr_data['text']):\n",
    "        # Clean the word, remove common punctuation, and handle case sensitivity\n",
    "        clean_word = word.strip().lower().replace('$', '').replace(',', '')\n",
    "        if clean_word:\n",
    "            words_and_coords.append({\n",
    "                'text': clean_word,\n",
    "                'left': ocr_data['left'][i],\n",
    "                'top': ocr_data['top'][i],\n",
    "                'width': ocr_data['width'][i],\n",
    "                'height': ocr_data['height'][i]\n",
    "            })\n",
    "\n",
    "    # Search for keywords and extract the next numeric value\n",
    "    for i, word_data in enumerate(words_and_coords):\n",
    "        text = word_data['text']\n",
    "        \n",
    "        # Look for the total amount\n",
    "        if text in ['total', 'balance', 'grandtotal']:\n",
    "            # The value is likely the next word after the keyword\n",
    "            for j in range(i + 1, min(i + 5, len(words_and_coords))):\n",
    "                next_word = words_and_coords[j]['text']\n",
    "                # Check if the next word is a number with an optional decimal\n",
    "                if re.match(r'^\\d+(\\.\\d+)?$', next_word):\n",
    "                    extracted_info['total'] = float(next_word)\n",
    "                    break # Exit the inner loop once the total is found\n",
    "        \n",
    "        # Look for subtotal\n",
    "        if text in ['subtotal', 'merchandise']:\n",
    "            for j in range(i + 1, min(i + 5, len(words_and_coords))):\n",
    "                next_word = words_and_coords[j]['text']\n",
    "                if re.match(r'^\\d+(\\.\\d+)?$', next_word):\n",
    "                    extracted_info['subtotal'] = float(next_word)\n",
    "                    break\n",
    "        \n",
    "        # Look for tax\n",
    "        if text in ['tax', 'gst', 'vat']:\n",
    "            for j in range(i + 1, min(i + 5, len(words_and_coords))):\n",
    "                next_word = words_and_coords[j]['text']\n",
    "                if re.match(r'^\\d+(\\.\\d+)?$', next_word):\n",
    "                    extracted_info['tax'] = float(next_word)\n",
    "                    break\n",
    "                    \n",
    "        # Look for a date\n",
    "        # We use a regex to find common date patterns (MM/DD/YYYY, DD-MM-YYYY, etc.)\n",
    "        if extracted_info['date'] is None:\n",
    "            # Re-check the raw, uncleaned text for dates\n",
    "            date_match = re.search(r'(\\d{1,2}[/\\-]\\d{1,2}[/\\-]\\d{2,4})', text)\n",
    "            if date_match:\n",
    "                extracted_info['date'] = date_match.group(1)\n",
    "\n",
    "    return extracted_info\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # --- Mock Data from Stage 2 ---\n",
    "    # In a real pipeline, this would be the output from\n",
    "    # pytesseract.image_to_data() on the preprocessed image.\n",
    "    mock_ocr_data = {\n",
    "        'level': [1, 2, 3, 4, 5, 5, 5, 5, 5, 5],\n",
    "        'page_num': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        'block_num': [1, 1, 1, 1, 2, 2, 2, 2, 2, 2],\n",
    "        'par_num': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        'line_num': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        'word_num': [1, 1, 1, 1, 1, 2, 3, 4, 5, 6],\n",
    "        'left': [100, 100, 100, 100, 300, 350, 400, 450, 500, 550],\n",
    "        'top': [50, 50, 50, 50, 150, 150, 150, 200, 200, 200],\n",
    "        'width': [50, 50, 50, 50, 60, 60, 60, 60, 60, 60],\n",
    "        'height': [20, 20, 20, 20, 20, 20, 20, 20, 20, 20],\n",
    "        'text': ['store', 'name', 'date', '2025/08/26', 'Total', '$', '45.75', 'Tax', '$', '3.15']\n",
    "    }\n",
    "    \n",
    "    print(\"Starting information extraction...\")\n",
    "    \n",
    "    # Run the extraction function on the mock OCR data\n",
    "    extracted_data = extract_receipt_info(mock_ocr_data)\n",
    "    \n",
    "    print(\"\\n--- Extracted Receipt Information ---\")\n",
    "    for key, value in extracted_data.items():\n",
    "        print(f\"{key.capitalize()}: {value}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8211b605",
   "metadata": {},
   "source": [
    "# GEN AI RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a087303",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d3df8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A script for the Retrieval-Augmented Generation (RAG) approach\n",
    "# to information extraction.\n",
    "\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# --- Stage 1: Mock Vector Database and Embedding Model ---\n",
    "# In a real-world application, this would be a full-fledged vector database\n",
    "# like ChromaDB, Pinecone, or a self-hosted solution.\n",
    "# The `embedding_model` would be a transformer model (e.g., Sentence-Transformers).\n",
    "\n",
    "def mock_embedding_model(text: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Simulates a text embedding model.\n",
    "    A real model would convert text into a high-dimensional vector.\n",
    "    For this example, we'll just create a simple, repeatable vector.\n",
    "    \"\"\"\n",
    "    # A simple, mock embedding for demonstration.\n",
    "    # The sum of ASCII values gives a unique-ish number for each string.\n",
    "    hash_value = sum(ord(char) for char in text)\n",
    "    return np.array([hash_value, hash_value / 100, hash_value % 100, 1.0])\n",
    "\n",
    "class MockVectorDatabase:\n",
    "    \"\"\"\n",
    "    Simulates a vector database that stores and retrieves text chunks.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.embeddings = []\n",
    "        self.chunks = []\n",
    "\n",
    "    def ingest_chunks(self, text_chunks: List[str]):\n",
    "        \"\"\"\n",
    "        Ingests a list of text chunks, creates embeddings, and stores them.\n",
    "        \"\"\"\n",
    "        for chunk in text_chunks:\n",
    "            embedding = mock_embedding_model(chunk)\n",
    "            self.embeddings.append(embedding)\n",
    "            self.chunks.append(chunk)\n",
    "    \n",
    "    def retrieve_similar_chunks(self, query_text: str, k: int = 3) -> List[str]:\n",
    "        \"\"\"\n",
    "        Retrieves the top-k most similar chunks to the query.\n",
    "        In a real system, this would use cosine similarity.\n",
    "        Here, we use a simple Euclidean distance on our mock vectors.\n",
    "        \"\"\"\n",
    "        query_embedding = mock_embedding_model(query_text)\n",
    "        \n",
    "        # Calculate a mock similarity score (e.g., inverse of distance)\n",
    "        # Note: This is for demonstration. Real vector search is more complex.\n",
    "        scores = [1.0 / (np.linalg.norm(query_embedding - emb) + 1e-6) for emb in self.embeddings]\n",
    "        \n",
    "        # Get indices of the top-k scores\n",
    "        top_k_indices = np.argsort(scores)[-k:][::-1]\n",
    "        \n",
    "        return [self.chunks[i] for i in top_k_indices]\n",
    "\n",
    "# --- Stage 2: Prompt Augmentation & LLM Generation ---\n",
    "\n",
    "def create_rag_prompt(query: str, retrieved_context: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Creates the augmented prompt for the LLM by adding retrieved context.\n",
    "\n",
    "    Args:\n",
    "        query (str): The original user query.\n",
    "        retrieved_context (List[str]): The relevant text chunks from the vector database.\n",
    "\n",
    "    Returns:\n",
    "        str: The full, augmented prompt string for the LLM.\n",
    "    \"\"\"\n",
    "    # Join the retrieved chunks into a single string\n",
    "    context_str = \"\\n\".join([f\"- {chunk}\" for chunk in retrieved_context])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are a highly accurate receipt data extractor. Your task is to analyze the provided OCR text from a receipt and extract key information.\n",
    "\n",
    "You have been provided with relevant text from the receipt to assist you.\n",
    "\n",
    "Please find the following details and return the output as a JSON object:\n",
    "- `total`: The final total amount of the receipt.\n",
    "- `subtotal`: The subtotal amount, if available.\n",
    "- `tax`: The tax amount, if available.\n",
    "- `date`: The date of the transaction.\n",
    "- `store_name`: The name of the store.\n",
    "\n",
    "If a value is not found, use `null`.\n",
    "\n",
    "Example JSON format:\n",
    "{{\n",
    "  \"store_name\": \"Example Store\",\n",
    "  \"total\": 45.75,\n",
    "  \"subtotal\": 42.60,\n",
    "  \"tax\": 3.15,\n",
    "  \"date\": \"2025/08/26\"\n",
    "}}\n",
    "\n",
    "---\n",
    "Relevant OCR Text Chunks (for context):\n",
    "{context_str}\n",
    "\n",
    "---\n",
    "JSON Output:\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def mock_llm_api_call(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    This function simulates a call to a local LLM API, similar to the previous example.\n",
    "    For this example, it returns a fixed JSON string.\n",
    "    \"\"\"\n",
    "    print(\"Sending augmented prompt to simulated local LLM...\")\n",
    "    \n",
    "    # The LLM's job is to use the provided context to fill in the JSON.\n",
    "    # The fixed response below simulates a correct extraction.\n",
    "    return \"\"\"\n",
    "{\n",
    "  \"store_name\": \"Groceries R Us\",\n",
    "  \"total\": 62.48,\n",
    "  \"subtotal\": 58.00,\n",
    "  \"tax\": 4.48,\n",
    "  \"date\": \"2025/08/26\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def extract_with_rag(ocr_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    The main RAG pipeline function. It retrieves relevant chunks and\n",
    "    then uses an LLM for generation.\n",
    "\n",
    "    Args:\n",
    "        ocr_data (dict): The output from the OCR stage.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with extracted information.\n",
    "    \"\"\"\n",
    "    # Create the mock vector database\n",
    "    db = MockVectorDatabase()\n",
    "    \n",
    "    # The \"chunks\" are just the words from the OCR text for this demo.\n",
    "    # In a real app, you would have a more sophisticated chunking strategy.\n",
    "    text_chunks = [word for word in ocr_data['text'] if word.strip()]\n",
    "    db.ingest_chunks(text_chunks)\n",
    "    \n",
    "    # 1. Retrieval Step: Find relevant chunks based on a query\n",
    "    # The query for the retriever can be a specific question or the raw OCR text itself.\n",
    "    retrieved_chunks = db.retrieve_similar_chunks(\"total tax subtotal store date\", k=5)\n",
    "    \n",
    "    print(\"Retrieved relevant chunks for augmentation:\")\n",
    "    print(retrieved_chunks)\n",
    "\n",
    "    # 2. Augmentation & Generation Step: Create the prompt and call the LLM\n",
    "    query = \"Extract store name, total, subtotal, tax, and date.\"\n",
    "    augmented_prompt = create_rag_prompt(query, retrieved_chunks)\n",
    "    \n",
    "    json_response_string = mock_llm_api_call(augmented_prompt)\n",
    "    \n",
    "    # 3. Parse the LLM's JSON output\n",
    "    try:\n",
    "        extracted_info = json.loads(json_response_string)\n",
    "        return extracted_info\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error parsing JSON from LLM: {e}\")\n",
    "        return {}\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # --- Mock OCR Data for Demonstration ---\n",
    "    # This simulates a different receipt than the previous example,\n",
    "    # showing the RAG system's ability to handle new data.\n",
    "    mock_ocr_data = {\n",
    "        'text': ['Welcome', 'to', 'Groceries', 'R', 'Us', '!', 'Subtotal', '58.00', 'Tax', '4.48', 'Total', '62.48', 'Thank', 'You', '!', '2025-08-26', '14:30'],\n",
    "    }\n",
    "\n",
    "    print(\"Starting RAG-based information extraction...\")\n",
    "    \n",
    "    extracted_data = extract_with_rag(mock_ocr_data)\n",
    "    \n",
    "    print(\"\\n--- Extracted Receipt Information (RAG) ---\")\n",
    "    if extracted_data:\n",
    "        for key, value in extracted_data.items():\n",
    "            print(f\"{key.capitalize()}: {value}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
